---
title: "Web scraping"
author: "Gaston Sanchez"
date: "July 30, 2015"
output: html_document
---

-----

## Getting Started with Web Scraping

Let's check the __Mail Code List__ pages from the _UC Berkeley Mail Services_ website: [http://mailservices.berkeley.edu/incoming/mailcodes/list](http://mailservices.berkeley.edu/incoming/mailcodes/list)

Note that the first page shows the departments "90.7 FM, AAS, ..., AHMA" and their respective codes. Then, at the bottom of the page you'll see a series of links numbered `1, 2, 3, 4, 5, 6, 7, 8, 9, ..., next> last>>`

__The purpose of this session is to scrape all the deparments and their mail codes.__

-----

## Looking at the HTML source code

Let's inspect the source html code. The first 3 departments (90.7 FM, AAS, and ACADEMIC ACHIEVEMENT PROGRAMS) appear in lines 209-211 within nodes like these:
```{r eval=FALSE}
<tr class="odd">
  <td class="view-field view-field-node-title active">90.7 FM</td>
  <td class="view-field view-field-node-data-field-code-field-code-value">5650</td> </tr>
<tr class="even">
  <td class="view-field view-field-node-title active">AAS</td>
  <td class="view-field view-field-node-data-field-code-field-code-value">2572</td> </tr>
<tr class="odd">
  <td class="view-field view-field-node-title active">ACADEMIC ACHIEVEMENT PROGRAMS</td>
  <td class="view-field view-field-node-data-field-code-field-code-value">2410</td> 
</tr>
```

We can match each department's name node with the following __XPath__ pattern:
```{r, eval=FALSE}
'//td[@class="view-field view-field-node-title active"]'
```

Likewise, we can match each department's code node with the following __XPath__ pattern:
```{r, eval=FALSE}
'//td[@class="view-field view-field-node-data-field-code-field-code-value"]'
```


-----

## Scraping Page 1

Let's scrape the first page:
```{r, eval=FALSE}
# load XML
library(XML)

# parsing html content
page1 <- htmlParse('http://mailservices.berkeley.edu/incoming/mailcodes/list')
```

How would you get the department names and codes?
```{r}
# department names
xpathSApply(
  doc = page1,
  path = '//td[@class="view-field view-field-node-title active"]',
  fun = xmlValue)
```

Likewise, we can match each department's code node with the following __XPath__ pattern:
```{r, eval=FALSE}
# department codes
xpathSApply(
  doc = page1,
  path = '//td[@class="view-field view-field-node-data-field-code-field-code-value"]',
  fun = xmlValue)
```


-----

## Scraping the first 5 pages

Instead of just scraping the first page, let's crawl through the first 5 pages: 1, 2, 3, 4, 5 

__Your mission is to obtain a data frame with two columns: `department` and `code`, and as many rows as departments.__

Brainstorm with your neighbors:

- How would you scrape the first 5 pages?
- Think about all the steps you need to perform
- Think about the data objects you may use to store the scraped content

```{r}
# empty vectors to store results
deps <- character(0)
codes <- numeric(0)

for (i in 0:4) {
  # parsing each page
  page <- htmlParse(paste0('http://mailservices.berkeley.edu/incoming/mailcodes/list?page=', i))

  # department names
  deps <- c(deps, xpathSApply(
    doc = page,
    path = '//td[@class="view-field view-field-node-title active"]',
    fun = xmlValue))

  # department codes
  codes <- c(codes, xpathSApply(
    doc = page,
    path = '//td[@class="view-field view-field-node-data-field-code-field-code-value"]',
    fun = xmlValue))
}

# assembling a data.frame
dep_code <- data.frame(
  department = deps,
  code = codes)
```

-----

## Scraping all the pages

Instead of just scraping the first 5 pages, you'll have to crawl through every single page: 1, 2, 3, 4, ... 

- How many pages you have to crawl over?
- Try to write functions that help you get the job done

```{r}
# scraping UC Berkeley Mail Codes



# ----
```
